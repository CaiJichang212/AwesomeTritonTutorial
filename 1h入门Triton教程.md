# Triton 1å°æ—¶å¿«é€Ÿå…¥é—¨æ•™ç¨‹

> é¢å‘Pythonç¨‹åºå‘˜çš„Triton GPUç¼–ç¨‹å…¥é—¨æŒ‡å—

---

## ğŸ“‹ æ•™ç¨‹å¤§çº²

| æ—¶é—´ | å†…å®¹ | ç›®æ ‡ |
|------|------|------|
| 0-10åˆ†é’Ÿ | ç¯å¢ƒæ­å»º & æ ¸å¿ƒæ¦‚å¿µ | ç†è§£Tritonæ˜¯ä»€ä¹ˆ |
| 10-25åˆ†é’Ÿ | ç¬¬ä¸€ä¸ªKernelï¼šå‘é‡åŠ æ³• | æŒæ¡åŸºæœ¬è¯­æ³• |
| 25-40åˆ†é’Ÿ | å®æˆ˜ï¼šçŸ©é˜µä¹˜æ³• | ç†è§£åˆ†å—è®¡ç®— |
| 40-55åˆ†é’Ÿ | ä¼˜åŒ–æŠ€å·§ & Softmaxå®ç° | æŒæ¡å¸¸ç”¨æ¨¡å¼ |
| 55-60åˆ†é’Ÿ | èµ„æº & ä¸‹ä¸€æ­¥å­¦ä¹  | æŒç»­å­¦ä¹ è·¯å¾„ |

---

## ç¬¬1éƒ¨åˆ†ï¼šç¯å¢ƒ & æ ¸å¿ƒæ¦‚å¿µï¼ˆ10åˆ†é’Ÿï¼‰

### 1.1 å®‰è£…

```bash
pip install triton torch
```

### 1.2 Triton æ˜¯ä»€ä¹ˆï¼Ÿ

```
ä¼ ç»ŸGPUç¼–ç¨‹ï¼šPython â†’ CUDA C++ â†’ GPUï¼ˆå­¦ä¹ æˆæœ¬é«˜ï¼‰
Tritonç¼–ç¨‹ï¼š  Python â†’ Triton â†’ GPUï¼ˆPythoné£æ ¼ï¼Œè‡ªåŠ¨ä¼˜åŒ–ï¼‰
```

### 1.3 æ ¸å¿ƒæ¦‚å¿µé€Ÿè®°

```python
"""
ğŸ”‘ ä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼š

1. Programï¼ˆç¨‹åºå®ä¾‹ï¼‰
   - æ¯ä¸ªprogramå¤„ç†æ•°æ®çš„ä¸€ä¸ª"å—"
   - ç±»ä¼¼äºCUDAçš„block

2. Blockï¼ˆæ•°æ®å—ï¼‰
   - TritonæŒ‰å—å¤„ç†æ•°æ®ï¼ˆå¦‚128/256/512ä¸ªå…ƒç´ ï¼‰
   - è‡ªåŠ¨å¤„ç†è¾¹ç•Œæƒ…å†µ

3. tlï¼ˆtriton.languageï¼‰
   - Tritonçš„æ ¸å¿ƒAPIåº“
   - æä¾›load/store/è®¡ç®—ç­‰æ“ä½œ
"""
```

---

## ç¬¬2éƒ¨åˆ†ï¼šç¬¬ä¸€ä¸ªKernel - å‘é‡åŠ æ³•ï¼ˆ15åˆ†é’Ÿï¼‰

### 2.1 å®Œæ•´ä»£ç 

```python
import torch
import triton
import triton.language as tl

# ============ Triton Kernel ============
@triton.jit  # ğŸ‘ˆ æ ¸å¿ƒè£…é¥°å™¨ï¼Œå°†Pythonå‡½æ•°ç¼–è¯‘ä¸ºGPUä»£ç 
def add_kernel(
    x_ptr,      # è¾“å…¥æŒ‡é’ˆ
    y_ptr,      # è¾“å…¥æŒ‡é’ˆ
    out_ptr,    # è¾“å‡ºæŒ‡é’ˆ
    n_elements, # å…ƒç´ æ€»æ•°
    BLOCK_SIZE: tl.constexpr,  # ğŸ‘ˆ ç¼–è¯‘æ—¶å¸¸é‡
):
    # 1ï¸âƒ£ è·å–å½“å‰programçš„IDï¼ˆç±»ä¼¼CUDAçš„blockIdxï¼‰
    pid = tl.program_id(axis=0)
    
    # 2ï¸âƒ£ è®¡ç®—å½“å‰blockå¤„ç†çš„å…ƒç´ ç´¢å¼•
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    
    # 3ï¸âƒ£ åˆ›å»ºmaskå¤„ç†è¾¹ç•Œï¼ˆé˜²æ­¢è¶Šç•Œè®¿é—®ï¼‰
    mask = offsets < n_elements
    
    # 4ï¸âƒ£ ä»GPUå†…å­˜åŠ è½½æ•°æ®
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    
    # 5ï¸âƒ£ è®¡ç®—
    output = x + y
    
    # 6ï¸âƒ£ å†™å›GPUå†…å­˜
    tl.store(out_ptr + offsets, output, mask=mask)


# ============ PythonåŒ…è£…å‡½æ•° ============
def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    # ç¡®ä¿è¾“å…¥åœ¨GPUä¸Š
    assert x.is_cuda and y.is_cuda
    output = torch.empty_like(x)
    n_elements = x.numel()
    
    # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªprogramï¼ˆgridå¤§å°ï¼‰
    BLOCK_SIZE = 1024
    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)  # å‘ä¸Šå–æ•´é™¤æ³•
    
    # å¯åŠ¨kernel
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE)
    
    return output


# ============ æµ‹è¯• ============
if __name__ == "__main__":
    torch.manual_seed(0)
    size = 98432  # æ•…æ„ä¸æ˜¯2çš„å¹‚æ¬¡ï¼Œæµ‹è¯•è¾¹ç•Œå¤„ç†
    x = torch.rand(size, device='cuda')
    y = torch.rand(size, device='cuda')
    
    # å¯¹æ¯”æµ‹è¯•
    triton_output = add(x, y)
    torch_output = x + y
    
    print(f"âœ… ç»“æœæ­£ç¡®: {torch.allclose(triton_output, torch_output)}")
    print(f"æœ€å¤§è¯¯å·®: {(triton_output - torch_output).abs().max()}")
```

### 2.2 å…³é”®è¯­æ³•è§£æ

```python
"""
ğŸ“Œ @triton.jit å†…éƒ¨å¯ç”¨çš„æ“ä½œï¼š

ä½ç½®è®¡ç®—ï¼š
  tl.program_id(axis)    â†’ è·å–å½“å‰programåœ¨æŒ‡å®šè½´çš„ID
  tl.arange(start, end)  â†’ åˆ›å»ºè¿ç»­æ•´æ•°åºåˆ— [start, end)

å†…å­˜æ“ä½œï¼š
  tl.load(ptr, mask)     â†’ ä»GPUå†…å­˜åŠ è½½æ•°æ®
  tl.store(ptr, val, mask) â†’ å†™å…¥GPUå†…å­˜

æ•°å­¦è¿ç®—ï¼š
  +, -, *, /             â†’ é€å…ƒç´ è¿ç®—
  tl.exp, tl.log, tl.sin â†’ æ•°å­¦å‡½æ•°
  tl.max, tl.sum         â†’ å½’çº¦æ“ä½œ

ç‰¹æ®Šï¼š
  tl.constexpr           â†’ æ ‡è®°ç¼–è¯‘æ—¶å¸¸é‡ï¼ˆå¦‚BLOCK_SIZEï¼‰
"""
```

---

## ç¬¬3éƒ¨åˆ†ï¼šçŸ©é˜µä¹˜æ³•ï¼ˆ15åˆ†é’Ÿï¼‰

### 3.1 åˆ†å—æ€æƒ³å›¾è§£

```
çŸ©é˜µ A (MÃ—K) @ çŸ©é˜µ B (KÃ—N) = çŸ©é˜µ C (MÃ—N)

åˆ†å—è®¡ç®—ç­–ç•¥ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aå— (BMÃ—BK) â”‚  @  â”‚ Bå— (BKÃ—BN) â”‚  =  â”‚ Cå— (BMÃ—BN) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“                   â†“                   â†“
   é€å—åŠ è½½            é€å—åŠ è½½            ç´¯åŠ ç»“æœ
```

### 3.2 ç®€åŒ–ç‰ˆçŸ©é˜µä¹˜æ³•

```python
@triton.jit
def matmul_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,  # Açš„æ­¥é•¿
    stride_bk, stride_bn,  # Bçš„æ­¥é•¿
    stride_cm, stride_cn,  # Cçš„æ­¥é•¿
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
):
    # 1ï¸âƒ£ ç¡®å®šå½“å‰programè´Ÿè´£Cçš„å“ªä¸ªå—
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)
    
    # 2ï¸âƒ£ è®¡ç®—å—å†…åç§»
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)
    
    # 3ï¸âƒ£ åˆå§‹åŒ–ç´¯åŠ å™¨
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    # 4ï¸âƒ£ æ²¿Kç»´åº¦å¾ªç¯ç´¯åŠ 
    for k in range(0, K, BLOCK_K):
        # åŠ è½½Açš„ä¸€ä¸ªå— [BLOCK_M, BLOCK_K]
        a_ptrs = a_ptr + offs_m[:, None] * stride_am + (k + offs_k[None, :]) * stride_ak
        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & ((k + offs_k[None, :]) < K), other=0.0)
        
        # åŠ è½½Bçš„ä¸€ä¸ªå— [BLOCK_K, BLOCK_N]
        b_ptrs = b_ptr + (k + offs_k[:, None]) * stride_bk + offs_n[None, :] * stride_bn
        b = tl.load(b_ptrs, mask=((k + offs_k[:, None]) < K) & (offs_n[None, :] < N), other=0.0)
        
        # å—çŸ©é˜µä¹˜æ³•ç´¯åŠ 
        acc += tl.dot(a, b)
    
    # 5ï¸âƒ£ å†™å›ç»“æœ
    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    tl.store(c_ptrs, acc, mask=mask)


def matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    M, K = a.shape
    K, N = b.shape
    c = torch.empty((M, N), device=a.device, dtype=a.dtype)
    
    # å—å¤§å°é…ç½®
    BLOCK_M, BLOCK_N, BLOCK_K = 64, 64, 32
    
    # 2D grid
    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))
    
    matmul_kernel[grid](
        a, b, c,
        M, N, K,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
        BLOCK_M, BLOCK_N, BLOCK_K,
    )
    return c


# æµ‹è¯•
if __name__ == "__main__":
    a = torch.randn(512, 256, device='cuda', dtype=torch.float16)
    b = torch.randn(256, 512, device='cuda', dtype=torch.float16)
    
    triton_out = matmul(a, b)
    torch_out = torch.matmul(a, b)
    
    print(f"âœ… ç»“æœæ­£ç¡®: {torch.allclose(triton_out, torch_out, atol=1e-2)}")
```

---

## ç¬¬4éƒ¨åˆ†ï¼šSoftmaxå®ç° & ä¼˜åŒ–æŠ€å·§ï¼ˆ15åˆ†é’Ÿï¼‰

### 4.1 Softmax Kernel

```python
@triton.jit
def softmax_kernel(
    input_ptr,
    output_ptr,
    n_cols,
    input_row_stride,
    output_row_stride,
    BLOCK_SIZE: tl.constexpr,
):
    # æ¯ä¸ªprogramå¤„ç†ä¸€è¡Œ
    row_idx = tl.program_id(0)
    
    # è®¡ç®—å½“å‰è¡Œçš„èµ·å§‹ä½ç½®
    row_start_ptr = input_ptr + row_idx * input_row_stride
    col_offsets = tl.arange(0, BLOCK_SIZE)
    
    # åŠ è½½ä¸€è¡Œæ•°æ®
    mask = col_offsets < n_cols
    row = tl.load(row_start_ptr + col_offsets, mask=mask, other=-float('inf'))
    
    # Softmaxè®¡ç®—ï¼ˆæ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼‰
    row_max = tl.max(row, axis=0)          # 1ï¸âƒ£ æ‰¾æœ€å¤§å€¼
    row = row - row_max                     # 2ï¸âƒ£ å‡æœ€å¤§å€¼ï¼ˆæ•°å€¼ç¨³å®šï¼‰
    numerator = tl.exp(row)                 # 3ï¸âƒ£ æŒ‡æ•°
    denominator = tl.sum(numerator, axis=0) # 4ï¸âƒ£ æ±‚å’Œ
    softmax_out = numerator / denominator   # 5ï¸âƒ£ å½’ä¸€åŒ–
    
    # å†™å›
    output_row_start_ptr = output_ptr + row_idx * output_row_stride
    tl.store(output_row_start_ptr + col_offsets, softmax_out, mask=mask)


def softmax(x: torch.Tensor) -> torch.Tensor:
    n_rows, n_cols = x.shape
    output = torch.empty_like(x)
    
    # BLOCK_SIZEå¿…é¡»æ˜¯2çš„å¹‚æ¬¡ä¸”>=n_cols
    BLOCK_SIZE = triton.next_power_of_2(n_cols)
    
    # æ¯è¡Œä¸€ä¸ªprogram
    grid = (n_rows,)
    
    softmax_kernel[grid](
        x, output, n_cols,
        x.stride(0), output.stride(0),
        BLOCK_SIZE,
    )
    return output
```

### 4.2 è‡ªåŠ¨è°ƒä¼˜ï¼ˆAutoTuneï¼‰

```python
@triton.autotune(
    configs=[
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_warps=4),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),
        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=8),
    ],
    key=['M', 'N', 'K'],  # æ ¹æ®è¿™äº›å‚æ•°é€‰æ‹©æœ€ä¼˜é…ç½®
)
@triton.jit
def matmul_autotune_kernel(...):
    # kernelä»£ç åŒä¸Š
    pass
```

### 4.3 æ€§èƒ½å¯¹æ¯”

```python
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['size'],
        x_vals=[2**i for i in range(10, 16)],
        line_arg='provider',
        line_vals=['triton', 'torch'],
        line_names=['Triton', 'PyTorch'],
        styles=[('blue', '-'), ('green', '-')],
        ylabel='GB/s',
        plot_name='vector-add-performance',
        args={},
    )
)
def benchmark(size, provider):
    x = torch.rand(size, device='cuda', dtype=torch.float32)
    y = torch.rand(size, device='cuda', dtype=torch.float32)
    
    if provider == 'triton':
        ms = triton.testing.do_bench(lambda: add(x, y))
    else:
        ms = triton.testing.do_bench(lambda: x + y)
    
    gbps = 3 * x.numel() * x.element_size() / ms * 1e-6
    return gbps

# è¿è¡Œ: benchmark.run(show_plots=True, print_data=True)
```

---

## ç¬¬5éƒ¨åˆ†ï¼šå¿«é€Ÿå‚è€ƒ & ä¸‹ä¸€æ­¥ï¼ˆ5åˆ†é’Ÿï¼‰

### 5.1 å¸¸ç”¨APIé€ŸæŸ¥

```python
"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Œ ä½ç½® & ç´¢å¼•
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
tl.program_id(axis)         # å½“å‰program ID
tl.num_programs(axis)       # programæ€»æ•°
tl.arange(start, end)       # è¿ç»­åºåˆ—

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Œ å†…å­˜æ“ä½œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
tl.load(ptr, mask, other)   # åŠ è½½ï¼Œotherä¸ºmask=Falseæ—¶çš„é»˜è®¤å€¼
tl.store(ptr, val, mask)    # å­˜å‚¨
tl.atomic_add(ptr, val)     # åŸå­åŠ 

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Œ æ•°å­¦è¿ç®—
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
tl.dot(a, b)                # çŸ©é˜µä¹˜æ³•
tl.exp, tl.log, tl.sqrt     # é€å…ƒç´ æ•°å­¦å‡½æ•°
tl.max, tl.min, tl.sum      # å½’çº¦æ“ä½œ
tl.where(cond, x, y)        # æ¡ä»¶é€‰æ‹©
tl.zeros, tl.full           # åˆ›å»ºå¼ é‡

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Œ è¾…åŠ©å‡½æ•°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
triton.cdiv(a, b)           # å‘ä¸Šå–æ•´é™¤æ³•
triton.next_power_of_2(n)   # ä¸‹ä¸€ä¸ª2çš„å¹‚æ¬¡
"""
```

### 5.2 å¸¸è§é”™è¯¯ & è§£å†³

```python
"""
âŒ é”™è¯¯ï¼šBLOCK_SIZEä¸æ˜¯2çš„å¹‚æ¬¡
âœ… è§£å†³ï¼šBLOCK_SIZE: tl.constexpr å¿…é¡»æ˜¯2çš„å¹‚æ¬¡

âŒ é”™è¯¯ï¼šå¿˜è®°maskå¯¼è‡´è¶Šç•Œ
âœ… è§£å†³ï¼šå§‹ç»ˆä½¿ç”¨ mask = offsets < n_elements

âŒ é”™è¯¯ï¼šç±»å‹ä¸åŒ¹é…
âœ… è§£å†³ï¼šæ˜¾å¼è½¬æ¢ x.to(tl.float32)

âŒ é”™è¯¯ï¼šgridè®¡ç®—é”™è¯¯
âœ… è§£å†³ï¼šä½¿ç”¨ triton.cdiv å‘ä¸Šå–æ•´
"""
```

### 5.3 å­¦ä¹ èµ„æº

| èµ„æº | é“¾æ¥ |
|------|------|
| ğŸ“š å®˜æ–¹æ•™ç¨‹ | https://triton-lang.org/main/getting-started/tutorials |
| ğŸ’» GitHub | https://github.com/openai/triton |
| ğŸ“– FlashAttentionå®ç° | å®˜æ–¹æ•™ç¨‹ç¬¬6èŠ‚ |
| ğŸ”§ å®æˆ˜é¡¹ç›® | Unslothã€xFormers |

---

## âœ… 1å°æ—¶å­¦ä¹ æ£€æŸ¥æ¸…å•

```
â–¡ ç†è§£ @triton.jit è£…é¥°å™¨çš„ä½œç”¨
â–¡ èƒ½è§£é‡Š program_id å’Œ BLOCK_SIZE çš„å…³ç³»
â–¡ æŒæ¡ tl.load / tl.store / mask çš„ä½¿ç”¨
â–¡ ç†è§£çŸ©é˜µä¹˜æ³•çš„åˆ†å—ç­–ç•¥
â–¡ ä¼šä½¿ç”¨ triton.autotune è‡ªåŠ¨è°ƒä¼˜
â–¡ æˆåŠŸè¿è¡Œå‘é‡åŠ æ³•ç¤ºä¾‹
```

---

**ğŸ‰ æ­å–œå®Œæˆå…¥é—¨ï¼** ä¸‹ä¸€æ­¥å»ºè®®ï¼šå°è¯•å®ç° LayerNorm æˆ– GELU æ¿€æ´»å‡½æ•°ã€‚